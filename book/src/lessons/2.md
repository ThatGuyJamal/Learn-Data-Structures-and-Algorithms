# Understanding Big O Notation

## Introduction

In this lesson, we will explore the concept of Big O Notation, which is a fundamental tool for analyzing the efficiency of algorithms.
By analyzing the efficiency of algorithms, we can identify performance bottlenecks in our code and make informed decisions about which algorithm to use based on the context and constraints.

## What is Big O Notation?

Big O Notation is a mathematical representation used to describe the time complexity and space complexity of an algorithm. It provides a high-level understanding of how an algorithm's runtime or space requirements grow relative to the input size.

### Key Points

- **Time Complexity**: Describes how the runtime of an algorithm changes as the input size increases.
- **Space Complexity**: Describes how the memory usage of an algorithm changes as the input size increases.
- **Asymptotic Analysis**: Focuses on the behavior of an algorithm as the input size approaches infinity.

## Common Big O Notations

Let's look at some common Big O Notations and what they mean in terms of algorithm efficiency.

### O(1) - Constant Time

An algorithm with O(1) complexity runs in constant time, regardless of the input size.

Constant Time means that the time it takes for the algorithm to complete does not change regardless of the size of the input.
In other words, whether you have 10 elements or 10 million elements, the algorithm will take the same amount of time.

Think of Constant Time like looking up a specific page in a phone book by going straight to that page. It doesn't matter how thick the phone book is; finding the page takes the same amount of time because you go directly to it without needing to check the other pages.

```rust
fn main() {
    let numbers = vec![10, 20, 30, 40, 50];

    let first = get_first_element(&numbers);

    println!("The first element is: {}", first);
}

fn get_first_element(vec: &Vec<i32>) -> i32 {
    vec[0] // Accessing the first element is a constant-time operation
}
```

In the main Function: We create a vector `numbers` with five elements. We then call `get_first_element`, which retrieves the first element of the vector. No matter how many elements are in the vector, the time it takes to get the first element remains constant.

Function `get_first_element`: This function takes a reference to a vector vec and returns the first element. Accessing an element by index in a vector is a constant-time operation because it directly accesses the memory location without needing to look through the entire vector.

### O(n) - Linear Time

An algorithm with O(n) complexity runs in linear time, meaning the runtime grows linearly with the input size.

Linear Time means that as the size of the input increases, the time it takes for the algorithm to complete increases proportionally. If the input size doubles, the time taken also roughly doubles.

Think of Linear Time like searching for a book in a library where the books are arranged in a single line. If you need to check each book one by one to find the one you're looking for, the time it takes will increase directly with the number of books you need to check.

```rust
fn main() {
    let numbers = vec![10, 20, 30, 40, 50];
    let target = 30;

    let found = find_element(&numbers, target);

    println!("Element found: {}", found);
}

fn find_element(vec: &Vec<i32>, target: i32) -> bool {
    for &item in vec.iter() {
        if item == target {
            return true; // Found the target
        }
    }
    false // Target not found
}
```

In the main Function: We create a vector `numbers` and call `find_element` to check if a specific number is present. The time taken to complete this operation depends on the number of elements in the vector.

Function `find_element`: This function takes a reference to a vector `vec` and a `target` number. It iterates through each element in the vector using a `for` loop. The time it takes to find the target increases linearly with the size of the vector.

### O(n^2) - Quadratic Time

An algorithm with O(n^2) complexity runs in quadratic time, meaning the runtime grows quadratically with the input size.

Quadratic Time means that if the size of the input doubles, the time it takes to run the algorithm will roughly increase by a factor of fou. This happens because the algorithm involves nested loops where each loop iterates over the size of the input.

Imagine you need to compare each pair of students in a class to find out which ones have the same birthday. If there are `n` students, you would need to perform `n * (n - 1) / 2` comparisons (approximately `n^2` comparisons) to ensure every pair is checked. As the number of students increases, the number of comparisons grows significantly.

```rust
fn main() {
    let mut numbers = vec![64, 25, 12, 22, 11];

    bubble_sort(&mut numbers);

    println!("Sorted array: {:?}", numbers);
}

fn bubble_sort(arr: &mut Vec<i32>) {
    let n = arr.len();
    for i in 0..n {
        for j in 0..n - 1 - i {
            if arr[j] > arr[j + 1] {
                arr.swap(j, j + 1);
            }
        }
    }
}
```

In the main Function: We create a vector `numbers` and sort it using the `bubble_sort` function. The sorting operation's time complexity is O(n^2).

Function `bubble_sort`: This function takes a mutable reference to a vector `arr` and sorts it using the bubble sort algorithm. The time complexity is quadratic because for each element in the outer loop, the inner loop iterates over most of the elements, resulting in roughly `n * (n - 1) / 2` comparisons and swaps.

### O(log n) - Logarithmic Time

An algorithm with O(log n) complexity runs in logarithmic time, meaning the runtime grows logarithmically with the input size.
This is common in algorithms that divide the problem in half each step, such as binary search.

Logarithmic Time means that as the input size doubles, the time required to complete the algorithm increases by a constant amount, rather than doubling.
This is because the algorithm reduces the problem size significantly with each step.

Imagine you're looking for a name in a phone book that is organized alphabetically. If you start in the middle and determine whether the name you're looking for is before or after that point, you effectively cut the number of names you need to search through in half with each step. This makes the search much faster compared to looking through every name one by one.

```rust
fn main() {
    let numbers = vec![10, 20, 30, 40, 50];
    let target = 30;

    match binary_search(&numbers, target) {
        Some(index) => println!("Element found at index: {}", index),
        None => println!("Element not found"),
    }
}

fn binary_search(arr: &Vec<i32>, target: i32) -> Option<usize> {
    let mut left = 0;
    let mut right = arr.len() as isize - 1;

    while left <= right {
        let mid = (left + right) / 2;
        let mid_value = arr[mid as usize];

        if mid_value == target {
            return Some(mid as usize);
        } else if mid_value < target {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }

    None // Target not found
}
```

In the main Function: We create a sorted vector `numbers` and use `binary_search` to find the target. The search operation's time complexity is O(log n).

Function `binary_search`: This function performs a binary search on a sorted vector `arr` to find the `target` value. The time complexity is logarithmic because the search is performed in half each step. It starts with the entire array (`left` to `right`). In each iteration, it calculates the midpoint and compares the `target` with the midpoint value. If the `target` is less than the midpoint, the search continues in the left half of the array. If the target is greater than the midpoint, the search continues in the right half of the array.

### O(n log n) - Linearithmic Time

An algorithm with O(n log n) complexity runs in linearithmic time, which is typical of efficient sorting algorithms like Merge Sort and Quick Sort.

Linearithmic Time means that the algorithm's runtime increases faster than Linear Time but slower than Quadratic Time.
It grows proportionally to the size of the input multiplied by the logarithm of the size of the input.

Imagine you have a large stack of unsorted papers that you want to sort. You divide the stack into smaller stacks, sort each smaller stack individually, and then combine them back together in order. The process of dividing, sorting, and merging each stack grows in a way that reflects Linearithmic Time complexity.

```rust
fn main() {
    let mut numbers = vec![38, 27, 43, 3, 9, 82, 10];

    merge_sort(&mut numbers);
    println!("Sorted array: {:?}", numbers);
}

fn merge_sort(arr: &mut Vec<i32>) {
    let len = arr.len();
    if len <= 1 {
        return;
    }

    let mid = len / 2;
    let mut left = arr[..mid].to_vec();
    let mut right = arr[mid..].to_vec();

    merge_sort(&mut left);
    merge_sort(&mut right);

    merge(arr, &left, &right);
}

fn merge(arr: &mut Vec<i32>, left: &Vec<i32>, right: &Vec<i32>) {
    let mut left_idx = 0;
    let mut right_idx = 0;
    let mut arr_idx = 0;

    while left_idx < left.len() && right_idx < right.len() {
        if left[left_idx] <= right[right_idx] {
            arr[arr_idx] = left[left_idx];
            left_idx += 1;
        } else {
            arr[arr_idx] = right[right_idx];
            right_idx += 1;
        }
        arr_idx += 1;
    }

    while left_idx < left.len() {
        arr[arr_idx] = left[left_idx];
        left_idx += 1;
        arr_idx += 1;
    }

    while right_idx < right.len() {
        arr[arr_idx] = right[right_idx];
        right_idx += 1;
        arr_idx += 1;
    }
}
```

In the main Function: We create a vector numbers and sort it using merge_sort. The sorting operation's time complexity is O(n log n).

Function `merge_sort`: This function performs the Merge Sort algorithm. It recursively divides the vector into two halves until each half is trivially sorted (i.e., has one or zero elements). It then merges the sorted halves using the `merge` function. The time complexity of the merge_sort function is O(n log n).

Function `merge`: This function merges two sorted arrays `left` and `right` into a single sorted array `arr`. The time complexity of the merge function is O(n).

## Why is Big O Notation Important?

Understanding Big O Notation helps you:

- Evaluate and compare the efficiency of different algorithms.
- Identify performance bottlenecks in your code.
- Make informed decisions about which algorithm to use based on the context and constraints.

### Conclusion

In this lesson, we've introduced Big O Notation and discussed its importance in analyzing algorithm efficiency. We also covered some common Big O Notations with Rust code examples. In the next lesson, we will dive into arrays and vectors, exploring their properties and usage in Rust.

Thank you for joining me in this journey to learn Rust and computer science concepts. Stay tuned for the next lesson!
